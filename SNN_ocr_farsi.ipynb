{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOSEPxOi0XJkLMVPxZarC9L",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Aliiior/snntorch/blob/master/SNN_ocr_farsi.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hYvuX6DZoWl2"
      },
      "outputs": [],
      "source": [
        "pip install snntorch -q\n",
        "pip install ranger-adabelief -q"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "HONNRMq0oaCy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test Accuracy: 98.43%\n",
        "\n",
        "1. Excitatory neurons (positive influence): Use a higher threshold of 1.0 and learning enabled\n",
        "\n",
        "2. Inhibitory neurons (negative influence): Use a lower threshold of 0.5 and no learning on the threshold\n",
        "\n",
        "3. Integrated combined spikes from excitatory and inhibitory neurons in the forward pass\n",
        "\n",
        "4. Increased Temporal Resolution\n",
        "\n",
        "- Reason: A higher number of time steps allows for better temporal dynamics and more accurate spike-based computation\n",
        "\n",
        "- Change: Increased num_steps from 50 to 60 for the spike sequence processing\n",
        "\n",
        "\n",
        "5. Enhanced Dropout Rates\n",
        "\n",
        "- Reason: Improved regularization to prevent overfitting during training\n",
        "\n",
        "- Change: Slightly increased dropout rates for better generalization\n",
        "\n",
        "... Dropout(0.5) for the first layer\n",
        "\n",
        "... Dropout(0.4) for the second layer\n",
        "\n",
        "\n",
        "\n",
        "6. Optimized Training with Mixed Precision\n",
        "\n",
        "- Added PyTorch AMP for mixed-precision training\n",
        "- Faster computation on GPUs\n",
        "- Reduced memory usage without losing accuracy\n",
        "- Integrated with GradScaler and torch.cuda.amp.autocast()\n",
        "\n",
        "7. Weight Initialization Improvements\n",
        "\n",
        "- Enhanced weight initialization for fully connected layers using Xavier Normal Initialization for better convergence:\n",
        "\n",
        "8. Optimizer and Scheduler\n",
        "\n",
        "- Used RangerAdaBelief for better generalization and optimization.\n",
        "- Scheduler: CosineAnnealingWarmRestarts for adaptive learning rate changes, encouraging better convergence during later epochs:"
      ],
      "metadata": {
        "id": "0YdsBuyypN6W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Imports\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from torchvision import transforms\n",
        "import snntorch as snn\n",
        "from snntorch import surrogate\n",
        "import numpy as np\n",
        "import random\n",
        "from ranger_adabelief import RangerAdaBelief\n",
        "\n",
        "# Set Random Seed\n",
        "seed = 42\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)\n",
        "np.random.seed(seed)\n",
        "random.seed(seed)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "# Class Distribution and Weights\n",
        "class_counts = {0.0: 19393, 1.0: 18148, 2.0: 15879, 3.0: 12010, 4.0: 11207,\n",
        "                5.0: 11993, 6.0: 11323, 7.0: 13356, 8.0: 13666, 9.0: 11851}\n",
        "total_samples = sum(class_counts.values())\n",
        "class_weights = {k: total_samples / v for k, v in class_counts.items()}\n",
        "weights = torch.tensor([class_weights[i] for i in range(10)], dtype=torch.float32)\n",
        "\n",
        "# Data Transformations\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize((28, 28)),\n",
        "    transforms.Grayscale(num_output_channels=1),\n",
        "    transforms.RandomRotation(15),\n",
        "    transforms.RandomAffine(0, scale=(0.8, 1.2)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.Resize((28, 28)),\n",
        "    transforms.Grayscale(num_output_channels=1),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "# Custom Dataset Class\n",
        "class FarsiDigitsDataset(Dataset):\n",
        "    def __init__(self, data, labels, transform=None):\n",
        "        self.data = data\n",
        "        self.labels = labels\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image = self.data[idx].squeeze()\n",
        "        label = self.labels[idx]\n",
        "        image = torch.tensor(image, dtype=torch.float32)\n",
        "        image = transforms.ToPILImage()(image)\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image, label\n",
        "\n",
        "# Load and Preprocess Data\n",
        "data1 = np.load('..../data_1241_before_preprocessing.npy')\n",
        "labels1 = np.load('.../labels_1241_before_preprocessing.npy')\n",
        "data2 = np.load('.../data_873_before_preprocessing.npy')\n",
        "labels2 = np.load('.../labels_873_before_preprocessing.npy')\n",
        "data3 = np.load('.../data_12610_before_preprocessing.npy')\n",
        "labels3 = np.load('.../labels_12610_before_preprocessing.npy')\n",
        "data4 = np.load('.../data_14579_before_preprocessing.npy')\n",
        "labels4 = np.load('.../labels_14579_before_preprocessing.npy')\n",
        "data5 = np.load('.../data_109523_before_preprocessing.npy')\n",
        "labels5 = np.load('.../labels_109523_before_preprocessing.npy')\n",
        "\n",
        "\n",
        "data = np.concatenate((data1, data2, data3,data4,data5), axis=0) / 255.0\n",
        "labels = np.concatenate((labels1, labels2, labels3,labels4,labels5), axis=0)\n",
        "\n",
        "# Dataset Splitting\n",
        "dataset = FarsiDigitsDataset(data, labels, transform=None)\n",
        "train_size = int(0.7 * len(dataset))\n",
        "val_size = int(0.15 * len(dataset))\n",
        "test_size = len(dataset) - train_size - val_size\n",
        "train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n",
        "\n",
        "train_dataset = FarsiDigitsDataset(train_dataset.dataset.data[train_dataset.indices],\n",
        "                                   train_dataset.dataset.labels[train_dataset.indices],\n",
        "                                   transform=train_transform)\n",
        "val_dataset = FarsiDigitsDataset(val_dataset.dataset.data[val_dataset.indices],\n",
        "                                 val_dataset.dataset.labels[val_dataset.indices],\n",
        "                                 transform=test_transform)\n",
        "test_dataset = FarsiDigitsDataset(test_dataset.dataset.data[test_dataset.indices],\n",
        "                                  test_dataset.dataset.labels[test_dataset.indices],\n",
        "                                  transform=test_transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=128, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)\n",
        "\n",
        "# Enhanced SNN Model (including exc / inh)\n",
        "\n",
        "class SNN_Enhanced(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SNN_Enhanced, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 32, 3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.layer_norm = nn.LayerNorm([64, 7, 7])\n",
        "\n",
        "        self.fc1 = nn.Linear(64 * 7 * 7, 512)\n",
        "        self.dropout1 = nn.Dropout(0.5)\n",
        "\n",
        "        self.fc2 = nn.Linear(512 * 10, 128)\n",
        "        self.dropout2 = nn.Dropout(0.4)\n",
        "\n",
        "        self.fc3 = nn.Linear(128, 10)\n",
        "\n",
        "        self.lif_exc = snn.Leaky(beta=0.9, spike_grad=surrogate.fast_sigmoid(), threshold=1.0, learn_threshold=True)\n",
        "        self.lif_inh = snn.Leaky(beta=0.85, spike_grad=surrogate.fast_sigmoid(), threshold=0.5, learn_threshold=False)\n",
        "\n",
        "        self.num_steps = 60\n",
        "        self.population_size = 10\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(torch.relu(self.conv1(x)))\n",
        "        x = self.layer_norm(self.pool(torch.relu(self.conv2(x))))\n",
        "\n",
        "        x = x.view(-1, 64 * 7 * 7)\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = x.repeat(1, self.population_size).view(-1, 512 * self.population_size)\n",
        "\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        x = self.dropout2(x)\n",
        "\n",
        "        x = x.unsqueeze(0).repeat(self.num_steps, 1, 1)\n",
        "        spk_rec = []\n",
        "\n",
        "        mem_exc, spk_exc = self.lif_exc.init_leaky(), self.lif_exc.init_leaky()\n",
        "        mem_inh, spk_inh = self.lif_inh.init_leaky(), self.lif_inh.init_leaky()\n",
        "\n",
        "        for step in range(self.num_steps):\n",
        "            spk_exc, mem_exc = self.lif_exc(x[step], mem_exc)\n",
        "            spk_inh, mem_inh = self.lif_inh(-x[step], mem_inh)\n",
        "            combined_spikes = spk_exc + spk_inh\n",
        "            cur = self.fc3(combined_spikes)\n",
        "            spk_rec.append(cur)\n",
        "\n",
        "        return torch.stack(spk_rec).sum(dim=0)\n",
        "\n",
        "# Training Configuration\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = SNN_Enhanced().to(device)\n",
        "loss_fn = nn.CrossEntropyLoss(weight=weights.to(device))\n",
        "optimizer = RangerAdaBelief(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
        "scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=2)\n",
        "\n",
        "# Training Loop\n",
        "best_model = None\n",
        "best_val_loss = float('inf')\n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "for epoch in range(50):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for data, labels in train_loader:\n",
        "        data, labels = data.to(device), labels.to(device, dtype=torch.int64)\n",
        "        with torch.cuda.amp.autocast():\n",
        "            spk_out = model(data)\n",
        "            loss = loss_fn(spk_out, labels)\n",
        "        optimizer.zero_grad()\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    scheduler.step()\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for data, labels in val_loader:\n",
        "            data, labels = data.to(device), labels.to(device, dtype=torch.int64)\n",
        "            spk_out = model(data)\n",
        "            val_loss += loss_fn(spk_out, labels).item()\n",
        "    val_loss /= len(val_loader)\n",
        "    print(f\"Epoch [{epoch+1}/50], Train Loss: {total_loss/len(train_loader):.4f}, Validation Loss: {val_loss:.4f}\")\n",
        "\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        best_model = model.state_dict()\n",
        "\n",
        "# Test Accuracy\n",
        "model.load_state_dict(best_model)\n",
        "model.eval()\n",
        "correct, total = 0, 0\n",
        "with torch.no_grad():\n",
        "    for data, labels in test_loader:\n",
        "        data, labels = data.to(device), labels.to(device, dtype=torch.int64)\n",
        "        spk_out = model(data)\n",
        "        preds = spk_out.argmax(dim=1)\n",
        "        correct += (preds == labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "\n",
        "print(f\"Test Accuracy: {100 * correct / total:.2f}%\")"
      ],
      "metadata": {
        "id": "JaXjQ3ReoaFa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "M9H_dEXLoaKS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Wryy23xcoaMx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}